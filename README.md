# Syst√®me de D√©tection de Fraude par Email

## üìã Description

Ce projet combine des approches d'apprentissage automatique et de mod√®les de langage pour d√©tecter les emails frauduleux √† partir du jeu de donn√©es Enron. Il utilise √† la fois des techniques de classification ML traditionnelles et une analyse bas√©e sur des mod√®les de langage pour identifier les communications potentiellement frauduleuses.

## üîß Pr√©requis

- Python 3.8+
- Ollama install√© localement
- Mod√®les de langage : phi, phi-3, mistral:7b, deepseak-r1:8b
- Jeu de donn√©es : `enron_data_fraud_labeled.csv`

## üöÄ Installation

1. Cloner le d√©p√¥t
2. Installer les d√©pendances :
    ```bash
    pip install -r requirements.txt
    ```
3. D√©marrer Ollama :
    ```bash
    ollama serve
    ```
4. T√©l√©charger le mod√®le de langage :
    ```bash
    ollama pull phi-3
    ```

## üíª Utilisation


### API Ollama

1. Ex√©cuter l'API Ollama :
    ```bash
    python script.py
    ```

### Entra√Ænement & Test

1. Ex√©cuter le classificateur (Pas tr√®s fonctionnel) :
    ```bash
    python classification.py
    ```

2. Tester le mod√®le de langage :
    ```bash
    python test_llm.py
    ```
    > **Note** : N√©cessite `enron_data_fraud_labeled.csv` dans le r√©pertoire racine

    Le script effectue les √©tapes suivantes :

    a. R√©cup√®re 9000 mails spam et 9000 mails non-spam, tri√©s en fonction des mails d√©j√† trait√©s.

    b. Teste ces mails avec le mod√®le de langage sp√©cifi√© au d√©but du script :
    ```python
    model = "phi3"
    ```
    c. Enregistre les r√©sultats tous les 10 mails dans un fichier `email_classification.db` contenant les informations suivantes :

    - `hash TEXT PRIMARY KEY` : Identifiant unique de l'email (pour le retrouver)
    - `sender TEXT` : Exp√©diteur de l'email
    - `subject TEXT` : Sujet de l'email
    - `body TEXT` : Corps de l'email
    - `poi_present INTEGER` : Indicateur de pr√©sence de point d'int√©r√™t
    - `classification TEXT` : Classification de l'email (spam/non-spam)
    - `rate TEXT` : Taux de pr√©cision de classification donn√© par le LLM (s'il est sur que l'email est spam, il est de 10, s'il est sur que l'email n'est pas spam, il est de 0)
    - `response_length INTEGER` : Longueur de la r√©ponse
    - `response_time REAL` : Temps de r√©ponse
    - `model TEXT` : Mod√®le de langage utilis√©


3. Voir les statistiques du mod√®le de langage :
    ```bash
    python stats_llm.py
    ```

    ### R√©sultats de l'analyse

    Voici un exemple des r√©sultats que vous pouvez obtenir en ex√©cutant `stats_llm.py` ici avec `phi3` comme mod√®le de langage :

    Nombre total d'emails analys√©s: 7530

    #### R√©sultats de l'analyse :
    - Nombre total d'√©chantillons: 7530
    - Nombre de pr√©dictions correctes: 3380
    - Pr√©cision globale: 44.89%

    #### Distribution des classifications :
    - OK: 4185 (55.58%)
    - NONOK: 3021 (40.12%)
    - UNKNOWN: 324 (4.30%)

    #### Analyse d√©taill√©e :

    Pour la classification NONOK :
    - Nombre total: 3021
    - Pr√©dictions correctes: 1439
    - Pr√©cision: 47.63%

    Pour la classification OK :
    - Nombre total: 4185
    - Pr√©dictions correctes: 1941
    - Pr√©cision: 46.38%

    #### Analyse d√©taill√©e des POI :

    Emails contenant des POI (poi_present = 1) :
    - Nombre total: 3714
    - Correctement identifi√©s comme NONOK: 1439
    - Pr√©cision: 38.75%

    Emails sans POI (poi_present = 0) :
    - Nombre total: 3816
    - Correctement identifi√©s comme OK: 1941
    - Pr√©cision: 50.86%

## üìÅ Structure du Projet

- `classification.py` : Entra√Ænement et √©valuation du classificateur ML
- `test_llm.py` : Module de test du mod√®le de langage
- `stats_llm.py` : Analyse des performances du mod√®le de langage
- `requirements.txt` : D√©pendances du projet

## üìä R√©sultats

Le syst√®me fournit :
- M√©triques de classification ML (pr√©cision, rappel, F1)
- R√©sultats et statistiques de l'analyse du mod√®le de langage
- Comparaison des performances entre les approches

## ‚ö†Ô∏è Notes Importantes

1. Assurez-vous qu'Ollama est en cours d'ex√©cution avant de tester le mod√®le de langage
2. Le fichier du jeu de donn√©es doit √™tre pr√©sent dans le r√©pertoire racine
3. Pour utiliser un jeu de donn√©es diff√©rent, mettez √† jour le chemin du fichier dans le code
4. Les r√©sultats sont stock√©s pour une analyse ult√©rieure