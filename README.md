# Syst√®me de D√©tection de Fraude par Email

## üìã Description

Ce projet combine des approches d'apprentissage automatique et de mod√®les de langage pour d√©tecter les emails frauduleux √† partir du jeu de donn√©es Enron. Il utilise √† la fois des techniques de classification ML traditionnelles et une analyse bas√©e sur des mod√®les de langage pour identifier les communications potentiellement frauduleuses.

## üîß Pr√©requis

- Python 3.8+
- Ollama install√© localement
- Mod√®les de langage : phi, phi-3, mistral:7b, deepseak-r1:8b
- Jeu de donn√©es : `enron_data_fraud_labeled.csv`

## üöÄ Installation

1. Cloner le d√©p√¥t
2. Installer les d√©pendances :
    ```bash
    pip install -r requirements.txt
    ```
3. D√©marrer Ollama :
    ```bash
    ollama serve
    ```
4. T√©l√©charger le mod√®le de langage :
    ```bash
    ollama pull phi-3
    ```

## üíª Utilisation


### API Ollama Documentation

#### 1. Lancement de l'API
```bash
python script.py
```
L'API sera accessible sur `http://localhost:8000` ou `http://127.0.0.1:8000`

#### 2. Points d'acc√®s (Endpoints)

**Classification des emails:**
- **POST** `/llm/{model}`
- Mod√®les disponibles: phi, phi3, mistral, deepseek
- Exemple: `http://localhost:8000/llm/phi3`

**V√©rification de sant√©:**
- **GET** `/health`
- Exemple: `http://localhost:8000/health`

#### 3. Format de la Requ√™te

**Headers:**
```
Content-Type: application/json
```

**Corps de la requ√™te (JSON):**
```json
{
    "sender": "john.doe@enron.com",
    "subject": "Meeting Tomorrow",
    "body": "Hello, Can we meet tomorrow at 2pm to discuss the project?"
}
```

#### 4. Format de la R√©ponse

**R√©ponse r√©ussie (200 OK):**
```json
{
    "classification": "OK",
    "rate": "2",
    "raw_response": "...",
    "json_response": {
        "CLASSIFICATION": "OK",
        "RATE": "2"
    }
}
```

**Classifications possibles:**
- `OK`: Email non frauduleux
- `NONOK`: Email potentiellement frauduleux
- `UNKNOWN`: Classification incertaine

**Rate:**
- √âchelle de 0 √† 10
- 0: Confiance maximale en `OK`
- 10: Confiance maximale en `NONOK`

#### 5. Exemples d'Utilisation

**Avec cURL:**
```bash
curl -X POST "http://localhost:8000/llm/phi3" \
     -H "Content-Type: application/json" \
     -d '{
           "sender": "john.doe@enron.com",
           "subject": "Meeting Tomorrow",
           "body": "Hello, Can we meet tomorrow at 2pm to discuss the project?"
         }'
```

**Avec Postman:**
1. Nouvelle requ√™te POST
2. URL: `http://localhost:8000/llm/phi3`
3. Headers: 
   - Key: `Content-Type`
   - Value: `application/json`
4. Body:
   - S√©lectionner "raw"
   - Type "JSON"
   - Copier le JSON d'exemple
5. Envoyer la requ√™te

### Entra√Ænement & Test

1. Ex√©cuter le classificateur (Pas tr√®s fonctionnel) :
    ```bash
    python classification.py
    ```

2. Tester le mod√®le de langage :
    ```bash
    python test_llm.py
    ```
    > **Note** : N√©cessite `enron_data_fraud_labeled.csv` dans le r√©pertoire racine

    Le script effectue les √©tapes suivantes :

    a. R√©cup√®re 9000 mails spam et 9000 mails non-spam, tri√©s en fonction des mails d√©j√† trait√©s.

    b. Teste ces mails avec le mod√®le de langage sp√©cifi√© au d√©but du script :
    ```python
    model = "phi3"
    ```
    c. Enregistre les r√©sultats tous les 10 mails dans un fichier `email_classification.db` contenant les informations suivantes :

    - `hash TEXT PRIMARY KEY` : Identifiant unique de l'email (pour le retrouver)
    - `sender TEXT` : Exp√©diteur de l'email
    - `subject TEXT` : Sujet de l'email
    - `body TEXT` : Corps de l'email
    - `poi_present INTEGER` : Indicateur de pr√©sence de point d'int√©r√™t
    - `classification TEXT` : Classification de l'email (spam/non-spam)
    - `rate TEXT` : Taux de pr√©cision de classification donn√© par le LLM (s'il est sur que l'email est spam, il est de 10, s'il est sur que l'email n'est pas spam, il est de 0)
    - `response_length INTEGER` : Longueur de la r√©ponse
    - `response_time REAL` : Temps de r√©ponse
    - `model TEXT` : Mod√®le de langage utilis√©


3. Voir les statistiques du mod√®le de langage :
    ```bash
    python stats_llm.py
    ```

    ### R√©sultats de l'analyse

    Voici un exemple des r√©sultats que vous pouvez obtenir en ex√©cutant `stats_llm.py` ici avec `phi3` comme mod√®le de langage :

    Nombre total d'emails analys√©s: 7530

    #### R√©sultats de l'analyse :
    - Nombre total d'√©chantillons: 7530
    - Nombre de pr√©dictions correctes: 3380
    - Pr√©cision globale: 44.89%

    #### Distribution des classifications :
    - OK: 4185 (55.58%)
    - NONOK: 3021 (40.12%)
    - UNKNOWN: 324 (4.30%)

    #### Analyse d√©taill√©e :

    Pour la classification NONOK :
    - Nombre total: 3021
    - Pr√©dictions correctes: 1439
    - Pr√©cision: 47.63%

    Pour la classification OK :
    - Nombre total: 4185
    - Pr√©dictions correctes: 1941
    - Pr√©cision: 46.38%

    #### Analyse d√©taill√©e des POI :

    Emails contenant des POI (poi_present = 1) :
    - Nombre total: 3714
    - Correctement identifi√©s comme NONOK: 1439
    - Pr√©cision: 38.75%

    Emails sans POI (poi_present = 0) :
    - Nombre total: 3816
    - Correctement identifi√©s comme OK: 1941
    - Pr√©cision: 50.86%

## üìÅ Structure du Projet

- `classification.py` : Entra√Ænement et √©valuation du classificateur ML
- `test_llm.py` : Module de test du mod√®le de langage
- `stats_llm.py` : Analyse des performances du mod√®le de langage
- `requirements.txt` : D√©pendances du projet

## üìä R√©sultats

Le syst√®me fournit :
- M√©triques de classification ML (pr√©cision, rappel, F1)
- R√©sultats et statistiques de l'analyse du mod√®le de langage
- Comparaison des performances entre les approches

## ‚ö†Ô∏è Notes Importantes

1. Assurez-vous qu'Ollama est en cours d'ex√©cution avant de tester le mod√®le de langage
2. Le fichier du jeu de donn√©es doit √™tre pr√©sent dans le r√©pertoire racine
3. Pour utiliser un jeu de donn√©es diff√©rent, mettez √† jour le chemin du fichier dans le code
4. Les r√©sultats sont stock√©s pour une analyse ult√©rieure